import os
import sys

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler  # ğŸ“Œ æ–°å¢å¯¼å…¥

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
from torch.utils.tensorboard import SummaryWriter
import torchvision.utils as vutils

from dataset.dataset_factory import get_dataset
from models.genericloss import GenericLoss
from models.model_tools import save_model
from models.total import Total
from utils.opts import opts


# ğŸ“Œ åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
def init_distributed_mode(opt):
    opt.rank = int(os.environ['RANK'])
    opt.local_rank = int(os.environ['LOCAL_RANK'])
    opt.world_size = int(os.environ['WORLD_SIZE'])

    dist.init_process_group(
        backend="nccl",
        init_method="env://",
        world_size=opt.world_size,
        rank=opt.rank
    )

    # ç¡®ä¿æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„GPU
    torch.cuda.set_device(opt.local_rank)
    opt.device = torch.device("cuda", opt.local_rank)

    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
    dist.barrier()


def main():
    opt = opts().parse()

    # ğŸ“Œ åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    if opt.distributed:  # æ·»åŠ æ–°çš„å‘½ä»¤è¡Œå‚æ•° --distributed
        init_distributed_mode(opt)
        print(f"Initialized distributed training on rank {opt.rank}")
    else:
        opt.rank = 0  # å•å¡æ¨¡å¼é»˜è®¤rankä¸º0
        opt.device = torch.device('cuda' if opt.gpus[0] >= 0 else 'cpu')

    # ğŸ“Œ åªåœ¨ä¸»è¿›ç¨‹åˆå§‹åŒ–TensorBoard
    if opt.rank == 0:
        writer = SummaryWriter(log_dir=f'logs/exp_{opt.exp_id}')
    else:
        writer = None

    # æ•°æ®é›†
    torch.manual_seed(opt.seed + opt.rank)  # ğŸ“Œ ä¸åŒrankè®¾ç½®ä¸åŒéšæœºç§å­
    Dataset = get_dataset(opt.dataset)
    opt = opts().update_dataset_info_and_set_heads(opt, Dataset)

    # ğŸ“Œ æ³¨é‡Šæ‰æ‰‹åŠ¨è®¾ç½®CUDA_VISIBLE_DEVICESï¼ˆç”±å¯åŠ¨è„šæœ¬æ§åˆ¶ï¼‰
    # if not opt.not_set_cuda_env:
    #     os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpus_str

    # ğŸ“Œ åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_dataset = Dataset(opt, 'train')
    sampler = DistributedSampler(
        train_dataset,
        num_replicas=opt.world_size if opt.distributed else 1,
        rank=opt.rank,
        shuffle=not opt.not_shuffle  # ğŸ“Œ é‡‡æ ·å™¨æ§åˆ¶shuffle
    ) if opt.distributed else None

    data_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=opt.batch_size,
        shuffle=(sampler is None),
        num_workers=opt.num_workers,
        pin_memory=True,  # ğŸ“Œ å»ºè®®å¼€å¯åŠ é€Ÿæ•°æ®ä¼ è¾“
        sampler=sampler,
        drop_last=True
    )

    # æ¨¡å‹
    model = Total(opt=opt)
    model = model.to(opt.device)

    # ğŸ“Œ DDPåŒ…è£…æ¨¡å‹
    if opt.distributed:
        model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)

    optimizer = torch.optim.Adam(model.parameters(), opt.lr)
    Loss = GenericLoss(opt=opt)

    # è®­ç»ƒ
    global_step = 0
    num_iters = len(data_loader) if opt.num_iters < 0 else opt.num_iters
    loss_min = sys.maxsize

    for epoch in range(opt.num_epochs):
        # ğŸ“Œ è®¾ç½®epochä¸ºsamplerçš„éšæœºç§å­
        if opt.distributed:
            data_loader.sampler.set_epoch(epoch)

        for iter_id, batch in enumerate(data_loader):
            if iter_id >= num_iters:
                break

            # ğŸ“Œ æ¸…é™¤æ˜¾å­˜æ”¹ä¸ºæŒ‰éœ€æ‰§è¡Œ
            if iter_id % 10 == 0:
                torch.cuda.empty_cache()

            # æ•°æ®è¿ç§»åˆ°è®¾å¤‡
            for k in batch:
                if k != 'meta':
                    batch[k] = batch[k].to(device=opt.device, non_blocking=True)

            # å‰å‘ä¼ æ’­
            pre_vi_img = batch.get('pre_vi_img', None)
            pre_ir_img = batch.get('pre_ir_img', None)
            pre_hm = batch.get('pre_hm', None)

            output = model(
                batch['vi_image'],
                batch['ir_image'],
                pre_vi_img,
                pre_ir_img,
                pre_hm
            )

            # è®¡ç®—æŸå¤±
            loss, loss_stats = Loss(output, batch)
            loss = loss.mean()

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # ğŸ“Œ åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜æ¨¡å‹å’Œè®°å½•æ—¥å¿—
            if opt.rank == 0:
                if loss.item() < loss_min:
                    loss_min = loss.item()
                    save_model(
                        model=model.module if opt.distributed else model,  # ğŸ“Œ æ³¨æ„è·å–åŸå§‹æ¨¡å‹
                        save_path='runs/best_model.pth',
                        epoch=epoch,
                        optimizer=optimizer
                    )

                # TensorBoardæ—¥å¿—è®°å½•
                if global_step % 100 == 0:
                    for name, value in loss_stats.items():
                        writer.add_scalar(f"Loss/{name}", value.mean(), global_step)
                    writer.add_scalar("Params/lr", optimizer.param_groups[0]['lr'], global_step)

                if global_step % 500 == 0:
                    with torch.no_grad():
                        vi_grid = vutils.make_grid(batch['vi_image'][:4], normalize=True)
                        ir_grid = vutils.make_grid(batch['ir_image'][:4], normalize=True)
                        fusion_grid = vutils.make_grid(output['fusion'][:4], normalize=True)
                        writer.add_image('Images/Visible', vi_grid, global_step)
                        writer.add_image('Images/Infrared', ir_grid, global_step)
                        writer.add_image('Images/Fusion', fusion_grid, global_step)

            global_step += 1
            del output, loss, loss_stats

    if opt.rank == 0 and writer is not None:
        writer.close()


if __name__ == "__main__":
    main()
